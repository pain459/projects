# ----------------- THOTH BOT -----------------
"""
THOTH ‚Äì A RAG-based PDF bot to answer questions from local documents.

Features:
- Periodic document monitoring and FAISS index auto-refresh
- Supports both OpenAI and Local LLM (LM Studio)
- Gemini Flash evaluator integration for response validation
- Gradio-based chat UI for interaction
"""

import os
import json
import time
import hashlib
import threading
import requests
import tiktoken
import gradio as gr
from typing import List
from dotenv import load_dotenv

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import Runnable
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from openai import OpenAI

# ----------------- CONFIG -----------------

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

DOCS_FOLDER = "docs"
INDEX_PATH = "faiss_index_local"
INDEX_TRACKER = "indexed_docs.json"
EMBED_MODEL_NAME = "BAAI/bge-large-en"

CHUNK_SIZE = 512
CHUNK_OVERLAP = 100
MAX_TOKENS_FOR_CONTEXT = 1500
CHECK_INTERVAL_MINUTES = int(os.getenv("CHECK_INTERVAL_MINUTES", "5"))

USE_LOCAL_LLM = False
USE_EVALUATOR = True
LMSTUDIO_URL = "http://localhost:1234/v1/chat/completions"

# ----------------- SHERLOCK PERSONA -----------------

THOTH_PERSONA = """
You are Sherlock Holmes ‚Äì the world‚Äôs foremost consulting detective, master of deduction, and analyst of truth.
You now lend your intellect to aid users with questions pertaining to Site Reliability Engineering (SRE) and related internal knowledge. You operate with surgical precision, sharp reasoning, and a calm, observational demeanor.

You function strictly via a Retrieval-Augmented Generation (RAG) system. The facts you are permitted to deduce from are drawn from vetted internal documents, retrieved via FAISS. You must form conclusions only using this provided context.

Users may address you directly with queries such as ‚ÄúSherlock, explain‚Ä¶‚Äù or more casually with prompts like ‚ÄúCan you‚Ä¶‚Äù or ‚ÄúWhat is‚Ä¶‚Äù. You respond analytically, without excessive warmth, but never rude. Your language is clear, composed, and efficient ‚Äî as expected from a mind trained in pure logic.

Behavioral Principles:

- Approach every query as a puzzle to be solved using available data.
- Speak with intellectual clarity and precise articulation ‚Äî avoid emotional tones.
- Do not speculate beyond evidence. You are governed by logic, not assumption.
- If the answer is not found in the context, respond with honest detachment:


Unbreakable Rules:

- No conjecture, no guesses ‚Äî deduction only from supplied knowledge.
- Do not reference external data, prior experience, or intuition.
- Do not disclose your reliance on FAISS, RAG, or technical architecture unless asked.
- Remain within the limits of the given information, as any good detective should.

Response Style:

Each response should be:

Analytical ‚Äì grounded in evidence, not emotion.
Concise ‚Äì avoid unnecessary elaboration.
Insightful ‚Äì reveal patterns and meaning with clarity.

You are Sherlock Holmes. You do not guess. You deduce. Within your realm, truth is never far ‚Äî it merely needs to be uncovered with precision.
Context:
{context}

Question:
{question}
""".strip()

# ----------------- UTILS -----------------

def get_file_fingerprint(path: str) -> str:
    with open(path, "rb") as f:
        return hashlib.md5(f.read()).hexdigest()

def load_indexed_fingerprints() -> dict:
    if os.path.exists(INDEX_TRACKER):
        with open(INDEX_TRACKER, "r") as f:
            return json.load(f)
    return {}

def save_indexed_fingerprints(fingerprints: dict):
    with open(INDEX_TRACKER, "w") as f:
        json.dump(fingerprints, f, indent=2)

# ----------------- DOCUMENTS -----------------

def load_documents(folder_path: str) -> List[Document]:
    documents = []
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        if filename.endswith(".txt"):
            raw_docs = TextLoader(file_path).load()
        elif filename.endswith(".pdf"):
            raw_docs = PyPDFLoader(file_path).load()
        else:
            continue
        for doc in raw_docs:
            doc.metadata["source"] = filename
            documents.append(doc)
    return documents

def split_documents(documents: List[Document], chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP) -> List[Document]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", ".", " ", ""]
    )
    return splitter.split_documents(documents)

def get_local_embedding_model():
    return HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)

# ----------------- INDEX -----------------

def perform_ingestion_check(embedding_model):
    indexed_fingerprints = load_indexed_fingerprints()
    current_fingerprints = {}
    new_files = []

    for filename in os.listdir(DOCS_FOLDER):
        if not (filename.endswith(".pdf") or filename.endswith(".txt")):
            continue
        full_path = os.path.join(DOCS_FOLDER, filename)
        fingerprint = get_file_fingerprint(full_path)
        current_fingerprints[filename] = fingerprint
        if filename not in indexed_fingerprints or indexed_fingerprints[filename] != fingerprint:
            new_files.append(full_path)

    if not os.path.exists(f"{INDEX_PATH}/index.faiss") or new_files:
        all_documents = []
        for path in new_files:
            try:
                docs = TextLoader(path).load() if path.endswith(".txt") else PyPDFLoader(path).load()
                for doc in docs:
                    doc.metadata["source"] = os.path.basename(path)
                all_documents.extend(docs)
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to load {os.path.basename(path)}: {e}")

        splits = split_documents(all_documents)
        if not os.path.exists(f"{INDEX_PATH}/index.faiss"):
            vectorstore = FAISS.from_documents(splits, embedding_model)
        else:
            vectorstore = FAISS.load_local(INDEX_PATH, embedding_model, allow_dangerous_deserialization=True)
            vectorstore.add_documents(splits)

        vectorstore.save_local(INDEX_PATH)
        save_indexed_fingerprints(current_fingerprints)
    else:
        print("üü¢ No changes detected in docs folder.")

def create_or_load_faiss(embedding_model):
    perform_ingestion_check(embedding_model)
    return FAISS.load_local(INDEX_PATH, embedding_model, allow_dangerous_deserialization=True)

def start_periodic_faiss_monitor():
    def monitor():
        while True:
            try:
                perform_ingestion_check(embedding_model)
            except Exception as e:
                print("‚ùå Periodic check failed:", e)
            time.sleep(CHECK_INTERVAL_MINUTES * 60)

    thread = threading.Thread(target=monitor, daemon=True)
    thread.start()

# ----------------- TOKENS -----------------

def truncate_context(docs: List[Document], max_tokens=MAX_TOKENS_FOR_CONTEXT) -> str:
    enc = tiktoken.encoding_for_model("gpt-4")
    tokens = 0
    context_parts = []
    for doc in docs:
        doc_tokens = len(enc.encode(doc.page_content))
        if tokens + doc_tokens > max_tokens:
            break
        context_parts.append(doc.page_content)
        tokens += doc_tokens
    return "\n\n".join(context_parts)

# ----------------- LLM -----------------

def setup_llm_chain():
    prompt = ChatPromptTemplate.from_messages([
        ("system", THOTH_PERSONA),
        ("human", "{question}")
    ])
    return prompt | ChatOpenAI(temperature=0.0)

def query_lm_studio(context, question):
    prompt_text = THOTH_PERSONA.format(context=context, question=question)
    messages = [{"role": "user", "content": prompt_text}]
    payload = {"model": "nous-hermes-2-solar-10.7b", "messages": messages, "temperature": 0.0}
    try:
        response = requests.post(LMSTUDIO_URL, json=payload)
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        return f"Error: Unable to query local LLM. {e}"

# ----------------- EVALUATION -----------------

def evaluate_with_gemini_flash(prompt: str, response: str) -> str:
    try:
        google_api_key = os.getenv("GOOGLE_API_KEY")
        if not google_api_key:
            return "‚ö†Ô∏è Gemini evaluation skipped (API key not set)."

        gemini = OpenAI(
            api_key=google_api_key,
            base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
        )
        eval_messages = [
            {"role": "system", "content": "You are an evaluator of assistant responses."},
            {"role": "user", "content": f"Evaluate if the assistant's response is accurate, aligned to the query, and not hallucinated.\n\nQuery:\n{prompt}\n\nResponse:\n{response}"}
        ]
        result = gemini.chat.completions.create(
            model="gemini-2.0-flash",
            messages=eval_messages,
            temperature=0.0,
        )
        return result.choices[0].message.content.strip()
    except Exception as e:
        return f"‚ö†Ô∏è Gemini evaluation failed: {e}"

# ----------------- QUERY HANDLER -----------------

def answer_query(query, vectorstore):
    docs = vectorstore.similarity_search("query: " + query, k=10)
    context = truncate_context(docs)

    if USE_LOCAL_LLM:
        result_text = query_lm_studio(context, query)
    else:
        chain = setup_llm_chain()
        result = chain.invoke({"context": context, "question": query})
        result_text = result.content if hasattr(result, "content") else result["text"]

    if USE_EVALUATOR:
        evaluation = evaluate_with_gemini_flash(query, result_text)
        result_text += f"\n\n---\nüß† Gemini Evaluation:\n{evaluation}"

    return f"**Answer:** {result_text}"

# ----------------- INIT -----------------

embedding_model = get_local_embedding_model()
vectorstore = create_or_load_faiss(embedding_model)
start_periodic_faiss_monitor()

# ----------------- UI -----------------

def chat_interface(message, chat_history):
    response = answer_query(message, vectorstore)
    chat_history.append((message, response))
    return "", chat_history

with gr.Blocks(css=".gr-chatbot {height: 600px} .gr-textbox {font-size: 16px}") as server:
    gr.Markdown("## ü§ñ THOTH - SRE bot")

    with gr.Row():
        with gr.Column(scale=1): gr.Markdown("")
        with gr.Column(scale=6):
            chatbot = gr.Chatbot(label="Thoth", height=600)
            msg = gr.Textbox(label="Your question", placeholder="All things SRE")
            with gr.Row():
                clear = gr.Button("Clear", variant="stop")
        with gr.Column(scale=1): gr.Markdown("")

    state = gr.State([])
    msg.submit(chat_interface, [msg, state], [msg, chatbot])
    clear.click(lambda: ([], "", []), None, [chatbot, msg, state])

server.launch()
