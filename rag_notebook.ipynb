{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d406ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\local\\src_git\\projects\\rag_pdf_bot\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "\n",
    "# Core packages\n",
    "import os\n",
    "import tempfile\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# pdf processing\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# vector search\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Text chunking and embedding\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# LLM and RAG\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Interface\n",
    "import gradio as gr\n",
    "\n",
    "# Token counting\n",
    "import tiktoken\n",
    "\n",
    "# loading environment variables from .env file\n",
    "\n",
    "load_dotenv()\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set in the environment variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e47a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF extraction and chunking\n",
    "\n",
    "def extract_pdf_text(file_path: str) -> str:\n",
    "    \"\"\"Extract raw text from a PDF file using pdfminer.\"\"\"\n",
    "    return extract_text(file_path)\n",
    "\n",
    "def split_text_to_chunks(text: str, chunk_size=1000, chunk_overlap=200) -> list:\n",
    "    \"\"\"Split text into overlapping chunks using LangChain's text splitter.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    return splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dfa88af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 646 text chunks from the PDF.\n",
      "Sample chunk:\n",
      " The \n",
      "\n",
      "Kubernetes \n",
      "\n",
      "Book\n",
      "\n",
      "2025 Edition\n",
      "\n",
      "Weapons-grade Kubernetes learning!\n",
      "\n",
      "Nigel Poulton @nigelpoulton\n",
      "\n",
      "\fAbout this edition\n",
      "\n",
      "This edition was published in February 2025.\n",
      "\n",
      "In writing this edition, I've\n"
     ]
    }
   ],
   "source": [
    "# sample run\n",
    "pdf_path = \"notes.pdf\"\n",
    "raw_text = extract_pdf_text(pdf_path)\n",
    "text_chunks = split_text_to_chunks(raw_text)\n",
    "\n",
    "print(f\"Extracted {len(text_chunks)} text chunks from the PDF.\")\n",
    "print(\"Sample chunk:\\n\", text_chunks[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3436526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding + FAISS Index creation\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def create_faiss_index(chunks: list, model_name: str = \"gpt-4o-mini\") -> FAISS:\n",
    "    \"\"\"\n",
    "    Generate embeddings for text chunks using a specific OpenAI embedding model,\n",
    "    then store them in a FAISS vector index.\n",
    "    \n",
    "    Default is 'text-embedding-3-small' (gpt-4o-mini embeddings).\n",
    "    \"\"\"\n",
    "    embedding_model = OpenAIEmbeddings(model=model_name)\n",
    "    vectorstore = FAISS.from_texts(text_chunks, embedding_model)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7e43828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\AppData\\Local\\Temp\\ipykernel_9732\\1048686540.py:3: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding_model = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "# Sample run\n",
    "# Creating a model instance\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# Use gpt-4o-mini's embedding model (text-embedding-3-small)\n",
    "vectorstore = create_faiss_index(text_chunks, model_name=\"text-embedding-3-small\")\n",
    "\n",
    "# Save for reuse\n",
    "vectorstore.save_local(\"faiss_index_store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0868e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Top-k Chunks and Build RAG Prompt\n",
    "\n",
    "def retrieve_relevant_chunks(vectorstore, query: str, k: int = 4) -> list:\n",
    "    \"\"\"Return top-k relevant text chunks for the query.\"\"\"\n",
    "    docs = vectorstore.similarity_search(query, k=k)\n",
    "    return [doc.page_content for doc in docs]\n",
    "\n",
    "def build_rag_prompt(query: str, context_chunks: list) -> str:\n",
    "    \"\"\"Format retrieved chunks and user query into a prompt for LLM.\"\"\"\n",
    "    context_text = \"\\n\\n---\\n\\n\".join(context_chunks)\n",
    "    prompt = f\"\"\"You are an expert assistant. Use the following context from a document to answer the user's question. If unsure, say so.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50779a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert assistant. Use the following context from a document to answer the user's question. If unsure, say so.\n",
      "\n",
      "Context:\n",
      "worker nodes not only allows the scheduler to spread your\n",
      "\n",
      "applications over multiple availability zones, but it may also\n",
      "\n",
      "render DoS attacks on any single node or zone ineﬀective (or\n",
      "\n",
      "less eﬀective).\n",
      "\n",
      "\fYou should also conﬁgure appropriate limits for the following:\n",
      "\n",
      "Memory\n",
      "\n",
      "CPU\n",
      "\n",
      "Storage\n",
      "\n",
      "Limits like these can help prevent essential system resources\n",
      "\n",
      "from being starved, therefore preventing potential DoS.\n",
      "\n",
      "Limiting Kubernetes objects can also be a good practice. This\n",
      "\n",
      "includes limiting things such as the number of ReplicaSets,\n",
      "\n",
      "Pods, Services, Secrets, and ConﬁgMaps in a particular\n",
      "\n",
      "Namespace.\n",
      "\n",
      "Here’s an example manifest that limits the number of Pod\n",
      "\n",
      "objects in the  skippy  Namespace to 100.\n",
      "\n",
      "apiVersion: v1\n",
      "\n",
      "kind: ResourceQuota\n",
      "metadata:\n",
      "\n",
      "  name: pod-quota\n",
      "  namespace: skippy\n",
      "\n",
      "spec:\n",
      "  hard:\n",
      "\n",
      "    pods: \"100\"\n",
      "\n",
      "One more feature —  podPidsLimit  — restricts the\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "# Step 1: Get top 4 chunks for the query\n",
    "query = \"Explain resource limits in Kubernetes.\"\n",
    "top_chunks = retrieve_relevant_chunks(vectorstore, query, k=4) # Adjust k for context length\n",
    "\n",
    "# Step 2: Build prompt for LLM\n",
    "prompt = build_rag_prompt(query, top_chunks)\n",
    "\n",
    "print(prompt[:1000])  # Preview prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72f69351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call OpenAI LLM with the formatted RAG prompt\n",
    "\n",
    "def get_llm_response(prompt: str, model_name: str = \"gpt-4o-mini\", temperature: float = 0.2) -> str:\n",
    "    \"\"\"\n",
    "    Sends the RAG prompt to the specified OpenAI Chat model and returns the response.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name: e.g., \"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4o\"\n",
    "    - temperature: controls randomness (0.0 = deterministic)\n",
    "\n",
    "    Returns:\n",
    "    - Response text from LLM\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(model=model_name, temperature=temperature)\n",
    "    response = llm([HumanMessage(content=prompt)])\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd3570a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\AppData\\Local\\Temp\\ipykernel_9732\\145465365.py:14: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model=model_name, temperature=temperature)\n",
      "C:\\Users\\kumar\\AppData\\Local\\Temp\\ipykernel_9732\\145465365.py:15: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm([HumanMessage(content=prompt)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Resource limits in Kubernetes are constraints set on the maximum amount of CPU and memory resources that a container within a Pod can use. These limits are specified in the Pod's YAML configuration under the `resources` section. The limits ensure that a container does not consume more resources than allocated, which helps maintain stability and performance across the cluster by preventing any single container from monopolizing resources.\n",
      "\n",
      "For example, in a Pod YAML configuration, you might see:\n",
      "\n",
      "```yaml\n",
      "resources:\n",
      "  limits:\n",
      "    cpu: 1.0\n",
      "    memory: 512Mi\n",
      "```\n",
      "\n",
      "This configuration sets a cap of one CPU and 512Mi of memory for the container. The Kubernetes runtime enforces these limits, ensuring that the container cannot exceed these specified resources. While a container can use more resources if available, it cannot surpass the defined limits, thus preventing resource starvation for other containers and potential denial of service (DoS) scenarios.\n"
     ]
    }
   ],
   "source": [
    "# Sample usage\n",
    "rag_response = get_llm_response(prompt, model_name=\"gpt-4o\")\n",
    "print(\"Answer:\\n\", rag_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96bf1df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio Interface for PDF QA Bot\n",
    "\n",
    "import gradio as gr\n",
    "from pathlib import Path\n",
    "\n",
    "# Globals to cache state\n",
    "VECTORSTORE = None\n",
    "CHUNKS = []\n",
    "PDF_NAME = None\n",
    "\n",
    "def process_pdf(file_obj, embedding_model: str = \"text-embedding-3-small\"):\n",
    "    \"\"\"Extracts text, creates chunks, and builds FAISS index from uploaded PDF.\"\"\"\n",
    "    global VECTORSTORE, CHUNKS, PDF_NAME\n",
    "\n",
    "    if not file_obj:\n",
    "        return \"❗ No file provided.\"\n",
    "\n",
    "    file_path = file_obj.name  # Gradio passes NamedString (with .name = path)\n",
    "    PDF_NAME = Path(file_path).stem\n",
    "\n",
    "    text = extract_pdf_text(file_path)\n",
    "    CHUNKS = split_text_to_chunks(text)\n",
    "    VECTORSTORE = create_faiss_index(CHUNKS, model_name=embedding_model)\n",
    "\n",
    "    return f\"✅ Processed {len(CHUNKS)} chunks from: {PDF_NAME}\"\n",
    "\n",
    "def handle_question(question: str, model: str = \"gpt-4o\"):\n",
    "    \"\"\"Handles the user query after PDF is processed.\"\"\"\n",
    "    if VECTORSTORE is None:\n",
    "        return \"❗ Please upload and process a PDF first.\"\n",
    "    \n",
    "    relevant = retrieve_relevant_chunks(VECTORSTORE, question, k=4)\n",
    "    prompt = build_rag_prompt(question, relevant)\n",
    "    answer = get_llm_response(prompt, model_name=model)\n",
    "    return answer\n",
    "\n",
    "# Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## 📄 RAG-based PDF QA Bot (OpenAI + FAISS)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        pdf_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
    "        embedding_model = gr.Textbox(label=\"Embedding Model\", value=\"text-embedding-3-small\")\n",
    "        process_btn = gr.Button(\"📚 Process PDF\")\n",
    "\n",
    "    status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "\n",
    "    with gr.Row():\n",
    "        question = gr.Textbox(label=\"Ask a question\")\n",
    "        model_choice = gr.Dropdown(choices=[\"gpt-4o\", \"gpt-4\", \"gpt-3.5-turbo\"], value=\"gpt-4o\", label=\"LLM Model\")\n",
    "        ask_btn = gr.Button(\"🔍 Get Answer\")\n",
    "\n",
    "    answer_output = gr.Textbox(label=\"Answer\", lines=8)\n",
    "\n",
    "    # Button actions\n",
    "    process_btn.click(process_pdf, inputs=[pdf_input, embedding_model], outputs=status)\n",
    "    ask_btn.click(handle_question, inputs=[question, model_choice], outputs=answer_output)\n",
    "\n",
    "# Launch the app\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7348b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_pdf_bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
