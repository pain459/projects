{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d406ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\local\\src_git\\projects\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "\n",
    "# Core packages\n",
    "import os\n",
    "import tempfile\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# pdf processing\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# vector search\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Text chunking and embedding\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# LLM and RAG\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Interface\n",
    "import gradio as gr\n",
    "\n",
    "# Token counting\n",
    "import tiktoken\n",
    "\n",
    "# loading environment variables from .env file\n",
    "\n",
    "load_dotenv()\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set in the environment variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e47a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF extraction and chunking\n",
    "\n",
    "def extract_pdf_text(file_path: str) -> str:\n",
    "    \"\"\"Extract raw text from a PDF file using pdfminer.\"\"\"\n",
    "    return extract_text(file_path)\n",
    "\n",
    "def split_text_to_chunks(text: str, chunk_size=1000, chunk_overlap=200) -> list:\n",
    "    \"\"\"Split text into overlapping chunks using LangChain's text splitter.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    return splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dfa88af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1697 text chunks from the PDF.\n",
      "Sample chunk:\n",
      " Site \n",
      "Reliability \n",
      "Engineering\n",
      "\n",
      "HOW GOOGLE RUNS PRODUCTION SYSTEMS\n",
      "\n",
      "Edited by Betsy Beyer, Chris Jones,  \n",
      "Jennifer Petoff & Niall Richard Murphy\n",
      "\n",
      "\f\fPraise for Site Reliability Engineering\n",
      "\n",
      "Google‚Äôs SR\n"
     ]
    }
   ],
   "source": [
    "# sample run\n",
    "pdf_path = \"notes.pdf\"\n",
    "raw_text = extract_pdf_text(pdf_path)\n",
    "text_chunks = split_text_to_chunks(raw_text)\n",
    "\n",
    "print(f\"Extracted {len(text_chunks)} text chunks from the PDF.\")\n",
    "print(\"Sample chunk:\\n\", text_chunks[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3436526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding + FAISS Index creation\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def create_faiss_index(chunks: list, model_name: str = \"gpt-4o-mini\") -> FAISS:\n",
    "    \"\"\"\n",
    "    Generate embeddings for text chunks using a specific OpenAI embedding model,\n",
    "    then store them in a FAISS vector index.\n",
    "    \n",
    "    Default is 'text-embedding-3-small' (gpt-4o-mini embeddings).\n",
    "    \"\"\"\n",
    "    embedding_model = OpenAIEmbeddings(model=model_name)\n",
    "    vectorstore = FAISS.from_texts(text_chunks, embedding_model)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7e43828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample run\n",
    "# Creating a model instance\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# Use gpt-4o-mini's embedding model (text-embedding-3-small)\n",
    "vectorstore = create_faiss_index(text_chunks, model_name=\"text-embedding-3-large\")  # small - 1536 vs large - 3072 dimensions\n",
    "\n",
    "\n",
    "# Save for reuse\n",
    "vectorstore.save_local(\"faiss_index_store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0868e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Top-k Chunks and Build RAG Prompt\n",
    "\n",
    "def retrieve_relevant_chunks(vectorstore, query: str, k: int = 4) -> list:\n",
    "    \"\"\"Return top-k relevant text chunks for the query.\"\"\"\n",
    "    docs = vectorstore.similarity_search(query, k=k)\n",
    "    return [doc.page_content for doc in docs]\n",
    "\n",
    "def build_rag_prompt(query: str, context_chunks: list) -> str:\n",
    "    \"\"\"Format retrieved chunks and user query into a prompt for LLM.\"\"\"\n",
    "    context_text = \"\\n\\n---\\n\\n\".join(context_chunks)\n",
    "    prompt = f\"\"\"You are an expert assistant. Use the following context from a document to answer the user's question. If unsure, say so.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50779a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert assistant. Use the following context from a document to answer the user's question. If unsure, say so.\n",
      "\n",
      "Context:\n",
      "Error Budgets\n",
      "Balance reliability and the pace of innovation with error budgets (see ‚ÄúMotivation for\n",
      "Error Budgets‚Äù on page 33), which define the acceptable level of failure for a service,\n",
      "over some period; we often use a month. A budget is simply 1 minus a service‚Äôs SLO;\n",
      "for  instance,  a  service  with  a  99.99%  availability  target  has  a  0.01%  ‚Äúbudget‚Äù  for\n",
      "unavailability.  As  long  as  the  service  hasn‚Äôt  spent  its  error  budget  for  the  month\n",
      "through the background rate of errors plus any downtime, the development team is\n",
      "free (within reason) to launch new features, updates, and so on.\n",
      "\n",
      "---\n",
      "\n",
      "Forming Your Error Budget\n",
      "In order to base these decisions on objective data, the two teams jointly define a quar‚Äê\n",
      "terly  error  budget  based  on  the  service‚Äôs  service  level  objective,  or  SLO  (see  Chap‚Äê\n",
      "ter  4).  The  error  budget  provides \n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "# Step 1: Get top 4 chunks for the query\n",
    "query = \"Explain error budgets in SRE.\"\n",
    "top_chunks = retrieve_relevant_chunks(vectorstore, query, k=4) # Adjust k for context length\n",
    "\n",
    "# Step 2: Build prompt for LLM\n",
    "prompt = build_rag_prompt(query, top_chunks)\n",
    "\n",
    "print(prompt[:1000])  # Preview prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72f69351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call OpenAI LLM with the formatted RAG prompt\n",
    "\n",
    "def get_llm_response(prompt: str, model_name: str = \"gpt-4o-mini\", temperature: float = 0.2) -> str:\n",
    "    \"\"\"\n",
    "    Sends the RAG prompt to the specified OpenAI Chat model and returns the response.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name: e.g., \"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4o\"\n",
    "    - temperature: controls randomness (0.0 = deterministic)\n",
    "\n",
    "    Returns:\n",
    "    - Response text from LLM\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(model=model_name, temperature=temperature)\n",
    "    response = llm([HumanMessage(content=prompt)])\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd3570a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Error budgets in Site Reliability Engineering (SRE) are a tool used to balance the reliability of a service with the pace of innovation. They define the acceptable level of failure for a service over a certain period, often a month or a quarter. An error budget is calculated as 1 minus the service's Service Level Objective (SLO). For example, if a service has a 99.99% availability target, it has a 0.01% error budget for unavailability.\n",
      "\n",
      "The error budget provides a clear, objective metric that determines how unreliable the service is allowed to be within a given period. This metric helps remove the politics from negotiations between SREs and product developers when deciding how much risk to allow. As long as the service hasn't exhausted its error budget, the development team is free to launch new features and updates.\n",
      "\n",
      "The benefits of an error budget include providing a common incentive for both product development and SRE to find the right balance between innovation and reliability. It allows product developers to take more risks when the budget is large and encourages them to be more cautious when the budget is nearly drained. If a network outage or datacenter failure reduces the measured SLO, it also eats into the error budget, potentially reducing the number of new pushes for the remainder of the period. This approach ensures that the entire team shares responsibility for maintaining uptime.\n"
     ]
    }
   ],
   "source": [
    "# Sample usage\n",
    "rag_response = get_llm_response(prompt, model_name=\"gpt-4o\")\n",
    "print(\"Answer:\\n\", rag_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96bf1df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio Interface for PDF QA Bot\n",
    "\n",
    "import gradio as gr\n",
    "from pathlib import Path\n",
    "\n",
    "# Globals to cache state\n",
    "VECTORSTORE = None\n",
    "CHUNKS = []\n",
    "PDF_NAME = None\n",
    "\n",
    "def process_pdf(file_obj, embedding_model: str = \"text-embedding-3-small\"):\n",
    "    \"\"\"Extracts text, creates chunks, and builds FAISS index from uploaded PDF.\"\"\"\n",
    "    global VECTORSTORE, CHUNKS, PDF_NAME\n",
    "\n",
    "    if not file_obj:\n",
    "        return \"‚ùó No file provided.\"\n",
    "\n",
    "    file_path = file_obj.name  # Gradio passes NamedString (with .name = path)\n",
    "    PDF_NAME = Path(file_path).stem\n",
    "\n",
    "    text = extract_pdf_text(file_path)\n",
    "    CHUNKS = split_text_to_chunks(text)\n",
    "    VECTORSTORE = create_faiss_index(CHUNKS, model_name=embedding_model)\n",
    "\n",
    "    return f\"‚úÖ Processed {len(CHUNKS)} chunks from: {PDF_NAME}\"\n",
    "\n",
    "def handle_question(question: str, model: str = \"gpt-4o\"):\n",
    "    \"\"\"Handles the user query after PDF is processed.\"\"\"\n",
    "    if VECTORSTORE is None:\n",
    "        return \"‚ùó Please upload and process a PDF first.\"\n",
    "    \n",
    "    relevant = retrieve_relevant_chunks(VECTORSTORE, question, k=4)\n",
    "    prompt = build_rag_prompt(question, relevant)\n",
    "    answer = get_llm_response(prompt, model_name=model)\n",
    "    return answer\n",
    "\n",
    "# Gradio UI\n",
    "with gr.Blocks() as server:\n",
    "    gr.Markdown(\"## üìÑ RAG-based PDF QA Bot (OpenAI + FAISS)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        pdf_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
    "        embedding_model = gr.Textbox(label=\"Embedding Model\", value=\"text-embedding-3-small\")\n",
    "        process_btn = gr.Button(\"üìö Process PDF\")\n",
    "\n",
    "    status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "\n",
    "    with gr.Row():\n",
    "        question = gr.Textbox(label=\"Ask a question\")\n",
    "        model_choice = gr.Dropdown(choices=[\"gpt-4o\", \"gpt-4\", \"gpt-3.5-turbo\"], value=\"gpt-4o\", label=\"LLM Model\")\n",
    "        ask_btn = gr.Button(\"üîç Get Answer\")\n",
    "\n",
    "    answer_output = gr.Textbox(label=\"Answer\", lines=8)\n",
    "\n",
    "    # Button actions\n",
    "    process_btn.click(process_pdf, inputs=[pdf_input, embedding_model], outputs=status)\n",
    "    ask_btn.click(handle_question, inputs=[question, model_choice], outputs=answer_output)\n",
    "\n",
    "# Launch the app\n",
    "server.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7348b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-pdf-bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
